modules = ["python-3.11", "nodejs-20"]
[agent]
expertMode = true

[nix]
channel = "stable-25_05"
packages = ["cargo", "glibcLocales", "libiconv", "libxcrypt", "pkg-config", "rapidfuzz-cpp", "rustc", "taskflow"]

[workflows]
runButton = "Project"

[[workflows.workflow]]
name = "Project"
mode = "parallel"
author = "agent"

[[workflows.workflow.tasks]]
task = "workflow.run"
args = "Recipe LLM Server"

[[workflows.workflow]]
name = "Recipe LLM Server"
author = "agent"

[workflows.workflow.metadata]
outputType = "webview"

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "RECIPE_MODE=minimal RECIPE_USE_MOCK=false RECIPE_MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0 python -m uvicorn app.main:app --host 0.0.0.0 --port 5000"
waitForPort = 5000

[[ports]]
localPort = 5000
externalPort = 80
